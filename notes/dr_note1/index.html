<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is it about these Deep research models lately ? | Akhil Pandey Akella</title>
<meta name="keywords" content="">
<meta name="description" content="Understanding deep research agents/models/queries/tasks | Part 1">
<meta name="author" content="Akhil Pandey">
<link rel="canonical" href="https://akhilpandey95.github.io/notes/dr_note1/">
<meta name="google-site-verification" content="G-PNYGCP0LEP">
<link crossorigin="anonymous" href="https://akhilpandey95.github.io/assets/css/stylesheet.cc07dec7dd9f28dca2ede3805b2043470261714d96bb4606b7d2b8e0b76d86f8.css" integrity="sha256-zAfex92fKNyi7eOAWyBDRwJhcU2Wu0YGt9K44Ldthvg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilpandey95.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilpandey95.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilpandey95.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilpandey95.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilpandey95.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilpandey95.github.io/notes/dr_note1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://akhilpandey95.github.io/css/extended/override.css">

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-PNYGCP0LEP"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-PNYGCP0LEP');
        }
      </script><meta property="og:url" content="https://akhilpandey95.github.io/notes/dr_note1/">
  <meta property="og:site_name" content="Akhil Pandey Akella">
  <meta property="og:title" content="What is it about these Deep research models lately ?">
  <meta property="og:description" content="Understanding deep research agents/models/queries/tasks | Part 1">
  <meta property="og:locale" content="en-US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-11-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-24T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What is it about these Deep research models lately ?">
<meta name="twitter:description" content="Understanding deep research agents/models/queries/tasks | Part 1">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://akhilpandey95.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is it about these Deep research models lately ?",
      "item": "https://akhilpandey95.github.io/notes/dr_note1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is it about these Deep research models lately ?",
  "name": "What is it about these Deep research models lately ?",
  "description": "Understanding deep research agents/models/queries/tasks | Part 1",
  "keywords": [
    
  ],
  "articleBody": "Lately there is an surge in explosion of models, recipes and software libraries that are capable of doing deep research. The nature of what constitutes as a deep-research task would really depend on the person you’re asking but its undeniable that any deep-research query is i.) agentic, ii..) long-horizon, iii.) large scale information seeking and iv.) information consumption workflow.\nDeep-research agents can be used for various search directives, but they scour the information at a considerably high depth, gather the context of all of the crawled information into a final answer that hopefully gives valuable insights.[1]. Inherently, this is a huge time and effort saving exercise if the report generated in the end is of high quality.\nSpecialized models The training behind DR Tulu is quite interesting because it is the first open-weight model that is post-trained using a new rubrics framework, or RLER as they describe it in their Github repo. To be very honest, the repository, blog and README in DR Tulu huggingface provide extensive information about the model and evaluation that its not worth it for me repeat the same information. Although, I personally feel the demo mentioned here gives you a sneak peek into final document quality. One of the cooler things you get to see in the demo page is the answers under section “SimpleQA”, because typically people expect deep research agents to have incredibly verbose answers when the purpose can sometimes be channeled towards rolling out terse answers but searching deeply.\nFig 1. Open deep research model list observed from DR Tulu web annoucement page. Learn more\nFor those interested in quickly training a deep-research model that is similar to DR Tulu, the instructions here highlight that testing the setup using Qwen-3 0.6B on a single (assuming H100) gpu is possible.\nTongyi DeepResearch takes a different approach to the deep research problem. It’s built on a Mixture-of-Experts (MoE) architecture with 30.5B total parameters but only 3.3B activated per token [5]. The training pipeline is elaborate: three distinct stages (Agentic CPT, SFT, RL) with a data synthesis framework called AgentFounder that generates training data from knowledge graphs. What stands out in their benchmarks is the performance on GAIA and AssistantBench–tasks that require multi-step reasoning across multiple sources. At inference time, you can switch between a lightweight ReAct mode for quick queries and IterResearch mode for thorough investigation, which feels like a practical acknowledgment that not every question needs the same depth.\nFig 2. Tongyi DeepResearch agentic model benchmark results on several search benchmarks. More available\nKosmos from Edison Scientific is architecturally the most distinctive of the three. Rather than betting on larger context windows or more sophisticated prompting, they built what they call a “structured world model”–a persistent, queryable database of entities, relationships, and open questions that survives across runs[6]. The system coordinates a data analysis agent and a literature search agent that share information through this store. The scale of operation is striking: a single research run can execute 42,000 lines of code and read 1,500 full papers. Their eval claims that one 20-cycle run was equivalent to 6 months of a human collaborator’s research time[3]. Perhaps more provocatively, they claim four novel contributions to scientific literature emerged from Kosmos runs–if that holds up under scrutiny, it’s a meaningful shift from aggregation to generation.\nFig 3. Input to output pipeline for a scientific world model involved in sophisticated autonomous discovery. More about Kosmos (https://edisonscientific.com/articles/announcing-kosmos) Structurally, are they different tho ? The appeal of a deep-research system comes down to how well it juggles searching, reading, and synthesizing. Looking at these three systems side by side, the philosophical divergences become clear.\nThe core tradeoff is between implicit knowledge (baked into weights via training) vs explicit knowledge (stored in external structures). DR Tulu bets heavily on training methodology–its architecture is deliberately minimal (just Qwen3-8B with protocol tokens like think, call_tool, cite, answer)[4]. The RLER approach keeps evolving rubrics during training, dropping ones that hit near-zero reward variance. The bet: you don’t need architectural complexity if your training signal is good enough.\nTongyi takes the opposite stance–scale is the answer, but make it efficient. The MoE architecture is fundamentally about having access to more parameters without paying for all of them at inference. The three-stage training pipeline (Agentic CPT → SFT → RL) suggests they view deep research capability as something that needs to be layered in progressively rather than emerging from a single training objective.\nKosmos sidesteps the question entirely. Instead of asking “how do we fit more knowledge into the model?”, they ask “why does knowledge need to live in the model at all?” The structured world model is essentially an admission that context windows are a fundamentally limited abstraction for research tasks. When you’re synthesizing across hundreds of papers, you need something that persists and can be queried structurally–not just a longer string of tokens.\nWhat’s interesting is how these bets align with different use cases. DR Tulu’s lightweight approach makes it accessible (trainable on a single GPU). Tongyi’s inference modes let you match depth to task complexity. Kosmos is overkill for simple queries but potentially necessary for actual scientific discovery. There’s no clear winner–just different points on the tradeoff surface.\nPersonally, I feel As a researcher, the biggest cognitive boost I can receive is by having a reliable co-scientist capable of understanding my workflows for consumption (web), knowledge updates (memory), and selective recall of consumed information (skills) at frequent/infrequent intervals such that I can play a productive role in directing research rather than drowning in its logistics.\nWhat excites me about the current landscape isn’t any single model–its the patterns emerging across all of them. The Kosmos eval where a single 20-cycle run was equivalent to 6 months of a collaborator’s research time[6] is cool, but how it gets there is cooler: a structured world model that just doesn’t forget. This hits at what I think is my biggest bottleneck–the cognitive overhead of re-establishing context every time I pick up a thread.\nTo be honest, I spend a non-trivial amount of time re-reading papers I’ve already read, re-deriving conclusions I’ve already drawn, rediscovering connections I already made. A system with persistent, queryable memory feels less like a tool and more like an extension of my episodic memory that doesn’t decay.\nThat said, I’m wary of systems where I can’t trace where claims come from. DR Tulu’s explicit cite token and Tongyi’s emphasis on faithful citations matter more to me than benchmark numbers. A convincingly-written but wrong paragraph is arguably worse than no output–its epistemic debt that compounds quietly.\nThe ideal co-scientist for me would probably blend Kosmos’s structured memory with DR Tulu’s citation-first approach and Tongyi’s inference flexibility (switching between quick ReAct responses and thorough IterResearch mode). We’re not there yet, but all three being open-weight means the community can iterate toward that.\nWhat I’m watching: whether these systems can move from aggregation to hypothesis generation that actually surprises domain experts. Kosmos claims four novel contributions to scientific literature[3]–if that holds up, we’re seeing the early stages of something qualitatively different from search-and-summarize.\nThe idea of “deep” is evolving One thing worth noting is that deep-research as a concept is getting productized and abstracted in interesting ways. Google’s Gemini Deep Research[7] exposes this as an API–an autonomous agent powered by Gemini 3 Pro that runs asynchronously through cycles of Plan -\u003e Search -\u003e Read -\u003e Iterate -\u003e Output. Tasks can take several minutes and run in the background, which is a fundamentally different UX from chat. They’re positioning it for “market analysis, due diligence, literature reviews, competitive landscaping”–basically anything where depth matters more than latency.\nOn the framework side, LangChain released Deep Agents[8]–a library inspired by tools like Claude Code and deep research systems. The interesting bits: built-in todo tracking for task decomposition, file system tools for offloading context to disk (preventing context window overflow), and the ability to spawn specialized subagents for specific tasks. Its basically a recognition that deep work requires different primitives than chat–planning, persistence, delegation.\nThe pattern I see emerging: deep-research is less about any single model architecture and more about an interaction paradigm. Long-running async execution, explicit planning phases, external memory stores, spawnable sub-agents. The models themselves (whether its DR Tulu, Tongyi, Gemini, or whatever powers a LangChain deep agent) are becoming somewhat interchangeable–what matters is the scaffolding around them. That scaffolding is what turns a capable LLM into something that can actually do research rather than just answer questions about research.\nReferences https://allenai.org/blog/dr-tulu https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/ https://edisonscientific.com/articles/announcing-kosmos https://arxiv.org/abs/2511.19399 https://arxiv.org/abs/2510.24701 https://arxiv.org/abs/2511.02824 https://ai.google.dev/gemini-api/docs/deep-research https://docs.langchain.com/oss/python/deepagents/overview ",
  "wordCount" : "1423",
  "inLanguage": "en",
  "datePublished": "2025-11-24T00:00:00Z",
  "dateModified": "2025-11-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Akhil Pandey"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilpandey95.github.io/notes/dr_note1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil Pandey Akella",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilpandey95.github.io/favicon.ico"
    }
  }
}
</script>
    
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      displayAlign: 'center',    /* truly center block math */
      displayIndent: '0em',      /* kill any default indent */
      "HTML-CSS": {
        styles: {'.MathJax_Preview': {visibility: 'hidden'}}
      }
    });
  </script>

  
  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
    async>
  </script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilpandey95.github.io/" accesskey="h" title="Akhil Pandey Akella (Alt + H)">Akhil Pandey Akella</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilpandey95.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://akhilpandey95.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://akhilpandey95.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://akhilpandey95.github.io/cv/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://akhilpandey95.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://akhilpandey95.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      What is it about these Deep research models lately ?
    </h1>
    <div class="post-description">
      Understanding deep research agents/models/queries/tasks | Part 1
    </div>
    <div class="post-meta"><span title='2025-11-24 00:00:00 +0000 UTC'>November 24, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Akhil Pandey

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#specialized-models" aria-label="Specialized models">Specialized models</a></li>
                <li>
                    <a href="#structurally-are-they-different-tho-" aria-label="Structurally, are they different tho ?">Structurally, are they different tho ?</a></li>
                <li>
                    <a href="#personally-i-feel" aria-label="Personally, I feel">Personally, I feel</a></li>
                <li>
                    <a href="#the-idea-of-deep-is-evolving" aria-label="The idea of &ldquo;deep&rdquo; is evolving">The idea of &ldquo;deep&rdquo; is evolving</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Lately there is an surge in explosion of models, recipes and software libraries that are capable of doing deep research. The nature of what constitutes as a <em>deep-research</em> task would really depend on the person you&rsquo;re asking but its undeniable that any <em>deep-research</em> query is i.) <strong>agentic</strong>, ii..) <strong>long-horizon</strong>, iii.) large scale <strong>information seeking</strong> and iv.) <strong>information consumption</strong> workflow.</p>
<p><em>Deep-research</em> agents can be used for various search directives, but they scour the information at a considerably high depth, gather the context of all of the crawled information into a final answer that hopefully gives valuable insights.<a href="https://allenai.org/blog/dr-tulu">[1]</a>. Inherently, this is a huge time and effort saving exercise if the report generated in the end is of high quality.</p>
<h3 id="specialized-models">Specialized models<a hidden class="anchor" aria-hidden="true" href="#specialized-models">#</a></h3>
<p>The training behind <em>DR Tulu</em> is quite interesting because it is the first open-weight model that is post-trained using a new rubrics framework, or RLER as they describe it in their Github repo. To be very honest, the repository, blog and README in <em>DR Tulu</em> huggingface provide extensive information about the model and evaluation that its not worth it for me repeat the same information. Although, I personally feel the demo mentioned <a href="https://dr-tulu.github.io/">here</a> gives you a sneak peek into final document quality. One of the cooler things you get to see in the demo page is the answers under section &ldquo;SimpleQA&rdquo;, because typically people expect deep research agents to have incredibly verbose answers when the purpose can sometimes be channeled towards rolling out terse answers but searching <em>deeply</em>.</p>
<figure>
    <a href="https://akhilpandey95.github.io/img/dr_model_list.png" target="_blank">
        <img src="https://akhilpandey95.github.io/img/dr_model_list.png" />
    </a>
    <figcaption>
        <p>Fig 1. Open deep research model list observed from DR Tulu web annoucement page. <a href="https://allenai.org/blog/dr-tulu" target="_blank">Learn more</a></p>
    </figcaption>
</figure>
<p>For those interested in quickly training a deep-research model that is similar to <em>DR Tulu</em>, the instructions <a href="https://github.com/rlresearch/dr-tulu/tree/main/rl/open-instruct">here</a> highlight that testing the setup using <strong>Qwen-3 0.6B</strong> on a single (assuming H100) gpu is possible.</p>
<p><em>Tongyi DeepResearch</em> takes a different approach to the deep research problem. It&rsquo;s built on a Mixture-of-Experts (MoE) architecture with 30.5B total parameters but only 3.3B activated per token <a href="https://arxiv.org/abs/2510.24701">[5]</a>. The training pipeline is elaborate: three distinct stages (Agentic CPT, SFT, RL) with a data synthesis framework called AgentFounder that generates training data from knowledge graphs. What stands out in their benchmarks is the performance on GAIA and AssistantBench&ndash;tasks that require multi-step reasoning across multiple sources. At inference time, you can switch between a lightweight ReAct mode for quick queries and IterResearch mode for thorough investigation, which feels like a practical acknowledgment that not every question needs the same depth.</p>
<figure>
    <a href="https://akhilpandey95.github.io/img/dr_tongyi_results.png" target="_blank">
        <img src="https://akhilpandey95.github.io/img/dr_tongyi_results.png" />
    </a>
    <figcaption>
        <p>Fig 2. Tongyi DeepResearch agentic model benchmark results on several search benchmarks. <a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/" target="_blank">More available</a></p>
    </figcaption>
</figure>
<p><em>Kosmos</em> from Edison Scientific is architecturally the most distinctive of the three. Rather than betting on larger context windows or more sophisticated prompting, they built what they call a &ldquo;structured world model&rdquo;&ndash;a persistent, queryable database of entities, relationships, and open questions that survives across runs<a href="https://arxiv.org/abs/2511.02824">[6]</a>. The system coordinates a data analysis agent and a literature search agent that share information through this store. The scale of operation is striking: a single research run can execute 42,000 lines of code and read 1,500 full papers. Their eval claims that one 20-cycle run was equivalent to 6 months of a human collaborator&rsquo;s research time<a href="https://edisonscientific.com/articles/announcing-kosmos">[3]</a>. Perhaps more provocatively, they claim four novel contributions to scientific literature emerged from Kosmos runs&ndash;if that holds up under scrutiny, it&rsquo;s a meaningful shift from aggregation to generation.</p>
<figure>
    <a href="https://akhilpandey95.github.io/img/kosmos.png" target="_blank">
        <img src="https://akhilpandey95.github.io/img/kosmos.png" />
    </a>
    <figcaption>
        <p>Fig 3. Input to output pipeline for a scientific world model involved in sophisticated autonomous discovery. More about Kosmos (https://edisonscientific.com/articles/announcing-kosmos)
    </figcaption>
</figure>
<h3 id="structurally-are-they-different-tho-">Structurally, are they different tho ?<a hidden class="anchor" aria-hidden="true" href="#structurally-are-they-different-tho-">#</a></h3>
<p>The appeal of a <em>deep-research</em> system comes down to how well it juggles searching, reading, and synthesizing. Looking at these three systems side by side, the philosophical divergences become clear.</p>
<p>The core tradeoff is between <em>implicit knowledge</em> (baked into weights via training) vs <em>explicit knowledge</em> (stored in external structures). DR Tulu bets heavily on training methodology&ndash;its architecture is deliberately minimal (just Qwen3-8B with protocol tokens like <code>think</code>, <code>call_tool</code>, <code>cite</code>, <code>answer</code>)<a href="https://arxiv.org/abs/2511.19399">[4]</a>. The RLER approach keeps evolving rubrics during training, dropping ones that hit near-zero reward variance. The bet: you don&rsquo;t need architectural complexity if your training signal is good enough.</p>
<p>Tongyi takes the opposite stance&ndash;scale <em>is</em> the answer, but make it efficient. The MoE architecture is fundamentally about having access to more parameters without paying for all of them at inference. The three-stage training pipeline (Agentic CPT → SFT → RL) suggests they view deep research capability as something that needs to be layered in progressively rather than emerging from a single training objective.</p>
<p>Kosmos sidesteps the question entirely. Instead of asking &ldquo;how do we fit more knowledge into the model?&rdquo;, they ask &ldquo;why does knowledge need to live in the model at all?&rdquo; The structured world model is essentially an admission that context windows are a fundamentally limited abstraction for research tasks. When you&rsquo;re synthesizing across hundreds of papers, you need something that persists and can be queried structurally&ndash;not just a longer string of tokens.</p>
<p>What&rsquo;s interesting is how these bets align with different use cases. DR Tulu&rsquo;s lightweight approach makes it accessible (trainable on a single GPU). Tongyi&rsquo;s inference modes let you match depth to task complexity. Kosmos is overkill for simple queries but potentially necessary for actual scientific discovery. There&rsquo;s no clear winner&ndash;just different points on the tradeoff surface.</p>
<h3 id="personally-i-feel">Personally, I feel<a hidden class="anchor" aria-hidden="true" href="#personally-i-feel">#</a></h3>
<p>As a researcher, the biggest cognitive boost I can receive is by having a reliable co-scientist capable of understanding my workflows for consumption (web), knowledge updates (memory), and selective recall of consumed information (skills) at frequent/infrequent intervals such that I can play a productive role in <em>directing</em> research rather than drowning in its logistics.</p>
<p>What excites me about the current landscape isn&rsquo;t any single model&ndash;its the patterns emerging across all of them. The Kosmos eval where a single 20-cycle run was equivalent to 6 months of a collaborator&rsquo;s research time<a href="https://arxiv.org/abs/2511.02824">[6]</a> is cool, but <em>how</em> it gets there is cooler: a structured world model that just doesn&rsquo;t forget. This hits at what I think is my biggest bottleneck&ndash;the cognitive overhead of re-establishing context every time I pick up a thread.</p>
<p>To be honest, I spend a non-trivial amount of time re-reading papers I&rsquo;ve already read, re-deriving conclusions I&rsquo;ve already drawn, rediscovering connections I already made. A system with persistent, queryable memory feels less like a tool and more like an extension of my episodic memory that doesn&rsquo;t decay.</p>
<p>That said, I&rsquo;m wary of systems where I can&rsquo;t trace where claims come from. DR Tulu&rsquo;s explicit <code>cite</code> token and Tongyi&rsquo;s emphasis on faithful citations matter more to me than benchmark numbers. A convincingly-written but wrong paragraph is arguably worse than no output&ndash;its epistemic debt that compounds quietly.</p>
<p>The ideal co-scientist for me would probably blend Kosmos&rsquo;s structured memory with DR Tulu&rsquo;s citation-first approach and Tongyi&rsquo;s inference flexibility (switching between quick ReAct responses and thorough IterResearch mode). We&rsquo;re not there yet, but all three being open-weight means the community can iterate toward that.</p>
<p>What I&rsquo;m watching: whether these systems can move from <em>aggregation</em> to <em>hypothesis generation</em> that actually surprises domain experts. Kosmos claims four novel contributions to scientific literature<a href="https://edisonscientific.com/articles/announcing-kosmos">[3]</a>&ndash;if that holds up, we&rsquo;re seeing the early stages of something qualitatively different from search-and-summarize.</p>
<h3 id="the-idea-of-deep-is-evolving">The idea of &ldquo;deep&rdquo; is evolving<a hidden class="anchor" aria-hidden="true" href="#the-idea-of-deep-is-evolving">#</a></h3>
<p>One thing worth noting is that <em>deep-research</em> as a concept is getting productized and abstracted in interesting ways. Google&rsquo;s <a href="https://ai.google.dev/gemini-api/docs/deep-research">Gemini Deep Research</a><a href="https://ai.google.dev/gemini-api/docs/deep-research">[7]</a> exposes this as an API&ndash;an autonomous agent powered by Gemini 3 Pro that runs asynchronously through cycles of <code>Plan -&gt; Search -&gt; Read -&gt; Iterate -&gt; Output</code>. Tasks can take several minutes and run in the background, which is a fundamentally different UX from chat. They&rsquo;re positioning it for &ldquo;market analysis, due diligence, literature reviews, competitive landscaping&rdquo;&ndash;basically anything where depth matters more than latency.</p>
<p>On the framework side, LangChain released <a href="https://docs.langchain.com/oss/python/deepagents/overview">Deep Agents</a><a href="https://docs.langchain.com/oss/python/deepagents/overview">[8]</a>&ndash;a library inspired by tools like Claude Code and deep research systems. The interesting bits: built-in todo tracking for task decomposition, file system tools for offloading context to disk (preventing context window overflow), and the ability to spawn specialized subagents for specific tasks. Its basically a recognition that <em>deep</em> work requires different primitives than chat&ndash;planning, persistence, delegation.</p>
<p>The pattern I see emerging: <em>deep-research</em> is less about any single model architecture and more about an <em>interaction paradigm</em>. Long-running async execution, explicit planning phases, external memory stores, spawnable sub-agents. The models themselves (whether its DR Tulu, Tongyi, Gemini, or whatever powers a LangChain deep agent) are becoming somewhat interchangeable&ndash;what matters is the scaffolding around them. That scaffolding is what turns a capable LLM into something that can actually do research rather than just answer questions about research.</p>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<ol>
<li><a href="https://allenai.org/blog/dr-tulu">https://allenai.org/blog/dr-tulu</a></li>
<li><a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></li>
<li><a href="https://edisonscientific.com/articles/announcing-kosmos">https://edisonscientific.com/articles/announcing-kosmos</a></li>
<li><a href="https://arxiv.org/abs/2511.19399">https://arxiv.org/abs/2511.19399</a></li>
<li><a href="https://arxiv.org/abs/2510.24701">https://arxiv.org/abs/2510.24701</a></li>
<li><a href="https://arxiv.org/abs/2511.02824">https://arxiv.org/abs/2511.02824</a></li>
<li><a href="https://ai.google.dev/gemini-api/docs/deep-research">https://ai.google.dev/gemini-api/docs/deep-research</a></li>
<li><a href="https://docs.langchain.com/oss/python/deepagents/overview">https://docs.langchain.com/oss/python/deepagents/overview</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://akhilpandey95.github.io/">Akhil Pandey Akella</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

