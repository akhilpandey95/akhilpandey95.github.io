<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is it about these Deep research models lately ? | Akhil Pandey Akella</title>
<meta name="keywords" content="">
<meta name="description" content="Understanding deep research agents/models/queries/tasks | Part 1">
<meta name="author" content="Akhil Pandey">
<link rel="canonical" href="https://akhilpandey95.github.io/notes/dr_note1/">
<meta name="google-site-verification" content="G-PNYGCP0LEP">
<link crossorigin="anonymous" href="https://akhilpandey95.github.io/assets/css/stylesheet.cc07dec7dd9f28dca2ede3805b2043470261714d96bb4606b7d2b8e0b76d86f8.css" integrity="sha256-zAfex92fKNyi7eOAWyBDRwJhcU2Wu0YGt9K44Ldthvg=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://akhilpandey95.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://akhilpandey95.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://akhilpandey95.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://akhilpandey95.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://akhilpandey95.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://akhilpandey95.github.io/notes/dr_note1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel="stylesheet" href="https://akhilpandey95.github.io/css/extended/override.css">

      <script async src="https://www.googletagmanager.com/gtag/js?id=G-PNYGCP0LEP"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-PNYGCP0LEP');
        }
      </script><meta property="og:url" content="https://akhilpandey95.github.io/notes/dr_note1/">
  <meta property="og:site_name" content="Akhil Pandey Akella">
  <meta property="og:title" content="What is it about these Deep research models lately ?">
  <meta property="og:description" content="Understanding deep research agents/models/queries/tasks | Part 1">
  <meta property="og:locale" content="en-US">
  <meta property="og:type" content="article">
    <meta property="article:section" content="notes">
    <meta property="article:published_time" content="2025-11-24T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-24T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="What is it about these Deep research models lately ?">
<meta name="twitter:description" content="Understanding deep research agents/models/queries/tasks | Part 1">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Notes",
      "item": "https://akhilpandey95.github.io/notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is it about these Deep research models lately ?",
      "item": "https://akhilpandey95.github.io/notes/dr_note1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is it about these Deep research models lately ?",
  "name": "What is it about these Deep research models lately ?",
  "description": "Understanding deep research agents/models/queries/tasks | Part 1",
  "keywords": [
    
  ],
  "articleBody": "Lately there is an surge in explosion of models, recipes and software libraries that are capable of doing deep research. The nature of what constitutes as a deep-research task would really depend on the person you’re asking but its undeniable that any deep-research query is i.) agentic, ii..) long-horizon, iii.) large scale information seeking and iv.) information consumption workflow.\nDeep-research agents can be used for various search directives, but they scour the information at a considerably high depth, gather the context of all of the crawled information into a final answer that hopefully gives valuable insights.[1]. Inherently, this is a huge time and effort saving exercise if the report generated in the end is of high quality.\nSpecialized models The training behind DR Tulu is quite interesting because it is the first open-weight model that is post-trained using a new rubrics framework, or RLER as they describe it in their Github repo. To be very honest, the repository, blog and README in DR Tulu huggingface provide extensive information about the model and evaluation that its not worth it for me repeat the same information. Although, I personally feel the demo mentioned here gives you a sneak peek into final document quality. One of the cooler things you get to see in the demo page is the answers under section “SimpleQA”, because typically people expect deep research agents to have incredibly verbose answers when the purpose can sometimes be channeled towards rolling out terse answers but searching deeply.\nFig 1. Open deep research model list observed from DR Tulu web annoucement page. Learn more\nFor those interested in quickly training a deep-research model that is similar to DR Tulu, the instructions here highlight that testing the setup using Qwen-3 0.6B on a single (assuming H100) gpu is possible.\nTongyi DeepResearch takes a different approach to the deep research problem. It’s built on a Mixture-of-Experts (MoE) architecture with 30.5B total parameters but only 3.3B activated per token [5]. The training pipeline is elaborate: three distinct stages (Agentic CPT, SFT, RL) with a data synthesis framework called AgentFounder that generates training data from knowledge graphs. What stands out in their benchmarks is the performance on GAIA and AssistantBench–tasks that require multi-step reasoning across multiple sources. At inference time, you can switch between a lightweight ReAct mode for quick queries and IterResearch mode for thorough investigation, which feels like a practical acknowledgment that not every question needs the same depth.\nFig 2. Tongyi DeepResearch agentic model benchmark results on several search benchmarks. More available\nKosmos from Edison Scientific is architecturally the most distinctive of the three. Rather than betting on larger context windows or more sophisticated prompting, they built what they call a “structured world model” a persistent, queryable database of entities, relationships, and open questions that survives across runs[6]. The system coordinates a data analysis agent and a literature search agent that share information through this store. The scale of operation is striking: a single research run can execute 42,000 lines of code and read 1,500 full papers. Their eval claims that one 20-cycle run was equivalent to 6 months of a human collaborator’s research time[3]. Perhaps more provocatively, they claim four novel contributions to scientific literature emerged from Kosmos runs if that holds up under scrutiny, it’s a meaningful shift from aggregation to generation.\nFig 3. Input to output pipeline for a scientific world model involved in sophisticated autonomous discovery. More about Kosmos (https://edisonscientific.com/articles/announcing-kosmos) Structurally, are they different tho ? The appeal of a deep-research system comes down to how well it juggles searching, reading, and synthesizing. Looking at these three systems side by side, the philosophical divergences become clear.\nWhile it can seem as the core tradeoff is between implicit knowledge vs explicit knowledge the RLER approach keeps evolving rubrics during training, dropping ones that hit near-zero reward variance because you don’t need a large complex model here if your training signal (for this specific task obviously) is good enough. I think i was very firm on this notion that if you treat this abstraction of think, call_tool, cite, answer [4] as a protocol, then training a model (like DR Tulu) for this task seems to be pretty straight forward to achieve good results.\nI guess operating with the above viewpoint made me question my core intuition when i noticed Tongyi’s model since it takes the opposite intuition because the report suggests scale and efficiency are the right choices for a performant model. The three stage training pipeline (Agentic CPT, SFT, RL) suggests they view deep research capability as something that needs to be layered in progressively rather than emerging from a single training objective.\nNow it might feel like I’m forcing in Kosmos into this discussion but if you give me a minute, I’d like to say that Kosmos sidesteps the above intuition entirely. Instead of asking “how do we fit more knowledge into the model?”, they ask “why does knowledge need to live in the model at all?” The structured world model is essentially an admission that context windows are a fundamentally limited abstraction for research tasks. When you’re synthesizing across hundreds of papers, you need something that persists and can be queried structurally. While Kosmos might be a commercial example of scientific product with structural difference on the appeal of a deep-research system, it’s interesting is how these three examples align with different use cases.\nDR Tulu’s lightweight approach makes it accessible (1 gpu) and focussed on a straight forward task. Tongyi’s model is more agentic and its work horse nature lets you match depth to task complexity while using the deep-research mode. Kosmos is overkill for simple queries but potentially necessary for scientific discovery or rather this promise of “scientific acceleration*. While this is not an classification encompassing all of the models/systems, these above three are definitely three interesting intuitions of a deep-research system that have an underlying abstraction thats possible to evolve.\nPersonally, I feel As a researcher, the biggest cognitive boost I can receive is by having a reliable co-scientist capable of understanding my workflows for consumption (web), knowledge updates (memory), and selective recall of consumed information (skills) at frequent/infrequent intervals such that I can play a productive role in directing research rather than drowning in its logistics.\nWhat excites me about the current landscape isn’t any single model–its the patterns emerging across all of them. The Kosmos eval where a single 20-cycle run was equivalent to 6 months of a collaborator’s research time[6] is cool, but how it gets there is cooler: a structured world model that just doesn’t forget. This hits at what I think is my biggest bottleneck particularly, the cognitive overhead of re-establishing context every time I pick up a thread. To be honest, I spend a non-trivial amount of time re-reading papers I’ve already read, re-deriving conclusions I’ve already drawn, rediscovering connections I already made. A system with persistent, queryable memory feels less like a tool and more like an extension of my episodic memory that doesn’t decay.\nA convincingly written but wrong paragraph is arguably worse than no output, so I kinda like systems where I can trace where claims come from, so DR Tulu’s explicit cite token and Tongyi’s emphasis on faithful citations matter more to me than benchmark numbers. The ideal co-scientist for me would probably a frankenstein blend of Kosmos’s structured memory with DR Tulu’s citation first approach and Tongyi’s inference flexibility. We’re not there yet (or we are and I’m unaware), but all three being open-weight means the community can iterate toward that.\nIt would be fascinating and super scary to see these systems move from aggregation to hypothesis generation that actually surprises domain experts. Kosmos claims four novel contributions to scientific literature[3] and if these contributions really hold up, we’re seeing the early stages of 2026 vibe-researcher / scientific acceleration phenomenon.\nThe idea of “deep” is evolving Deep-research as a concept is evolving and abstracted into many interesting ways. Google’s Gemini Deep Research[7] are some of the early examples of this evolving trend of deep agents powered by a strong foundational model capable on running long horizon tasks asynchronously through cycles of Plan -\u003e Search -\u003e Read -\u003e Iterate -\u003e Output. There are many variations of low-moderate level cognitive task that simply require longer horizons and this can be market analysis, scientific due diligence for drug discovery, carefull literature reviews for researchers to write a survey report/related work section in a paper, effectively any place where depth matters more than latency.\nAlso, its worth noting that open software SDKs/platforms are quickly converging on this notion because back in July'25 Deep Agents[8] was launched effectively giving proper software scaffolding for abstracting the idea of deep-agents. Ideally, its easy to have seen this coming given major strides (i mean culturally) in 2025 have come from gemini deep research, TUIs (claude code, codex), manus computer use agents and several other agentic products. deepagents package simply packages these into a reusable software scaffolding/harness with write_todos for task decomposition, file system tools (ls, read_file, write_file, edit_file) for offloading context to disk, and a task tool for spawning specialized subagents with context isolation.\nMost crucial emerging pattern i observe here is, deep-research being less about a single model (or) chat platform offering a button with “deep-research” but a gateway into a more sophisticated interaction pattern (or paradigm maybe). This slowly gives us a speak into 2026 on how enterprise software is going to reign in on long runnign agentic tasks; Long-running async execution, explicit planning phases, external memory stores, spawnable sub-agents. While its hard to see if open-source/open-weight efforts like DR Tulu, Tongyi will catch up to this evolving abstraction, a relevant point here is the seamless interchangable nature of “deep research” will practically define “what is a useful LLM?” into something that can actually do research rather than just answer questions about research.\nReferences https://allenai.org/blog/dr-tulu https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/ https://edisonscientific.com/articles/announcing-kosmos https://arxiv.org/abs/2511.19399 https://arxiv.org/abs/2510.24701 https://arxiv.org/abs/2511.02824 https://ai.google.dev/gemini-api/docs/deep-research https://blog.langchain.com/deep-agents/ ",
  "wordCount" : "1653",
  "inLanguage": "en",
  "datePublished": "2025-11-24T00:00:00Z",
  "dateModified": "2025-11-24T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Akhil Pandey"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://akhilpandey95.github.io/notes/dr_note1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Akhil Pandey Akella",
    "logo": {
      "@type": "ImageObject",
      "url": "https://akhilpandey95.github.io/favicon.ico"
    }
  }
}
</script>
    
  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\\[','\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      displayAlign: 'center',    /* truly center block math */
      displayIndent: '0em',      /* kill any default indent */
      "HTML-CSS": {
        styles: {'.MathJax_Preview': {visibility: 'hidden'}}
      }
    });
  </script>

  
  <script
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
    async>
  </script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://akhilpandey95.github.io/" accesskey="h" title="Akhil Pandey Akella (Alt + H)">Akhil Pandey Akella</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://akhilpandey95.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://akhilpandey95.github.io/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="https://akhilpandey95.github.io/notes/" title="Notes">
                    <span>Notes</span>
                </a>
            </li>
            <li>
                <a href="https://akhilpandey95.github.io/cv/cv.pdf" title="CV">
                    <span>CV</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://akhilpandey95.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://akhilpandey95.github.io/notes/">Notes</a></div>
    <h1 class="post-title entry-hint-parent">
      What is it about these Deep research models lately ?
    </h1>
    <div class="post-description">
      Understanding deep research agents/models/queries/tasks | Part 1
    </div>
    <div class="post-meta"><span title='2025-11-24 00:00:00 +0000 UTC'>November 24, 2025</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Akhil Pandey

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#specialized-models" aria-label="Specialized models">Specialized models</a></li>
                <li>
                    <a href="#structurally-are-they-different-tho-" aria-label="Structurally, are they different tho ?">Structurally, are they different tho ?</a></li>
                <li>
                    <a href="#personally-i-feel" aria-label="Personally, I feel">Personally, I feel</a></li>
                <li>
                    <a href="#the-idea-of-deep-is-evolving" aria-label="The idea of &ldquo;deep&rdquo; is evolving">The idea of &ldquo;deep&rdquo; is evolving</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Lately there is an surge in explosion of models, recipes and software libraries that are capable of doing deep research. The nature of what constitutes as a <em>deep-research</em> task would really depend on the person you&rsquo;re asking but its undeniable that any <em>deep-research</em> query is i.) <strong>agentic</strong>, ii..) <strong>long-horizon</strong>, iii.) large scale <strong>information seeking</strong> and iv.) <strong>information consumption</strong> workflow.</p>
<p><em>Deep-research</em> agents can be used for various search directives, but they scour the information at a considerably high depth, gather the context of all of the crawled information into a final answer that hopefully gives valuable insights.<a href="https://allenai.org/blog/dr-tulu">[1]</a>. Inherently, this is a huge time and effort saving exercise if the report generated in the end is of high quality.</p>
<h3 id="specialized-models">Specialized models<a hidden class="anchor" aria-hidden="true" href="#specialized-models">#</a></h3>
<p>The training behind <em>DR Tulu</em> is quite interesting because it is the first open-weight model that is post-trained using a new rubrics framework, or RLER as they describe it in their Github repo. To be very honest, the repository, blog and README in <em>DR Tulu</em> huggingface provide extensive information about the model and evaluation that its not worth it for me repeat the same information. Although, I personally feel the demo mentioned <a href="https://dr-tulu.github.io/">here</a> gives you a sneak peek into final document quality. One of the cooler things you get to see in the demo page is the answers under section &ldquo;SimpleQA&rdquo;, because typically people expect deep research agents to have incredibly verbose answers when the purpose can sometimes be channeled towards rolling out terse answers but searching <em>deeply</em>.</p>
<figure>
    <a href="https://akhilpandey95.github.io/img/dr_model_list.png" target="_blank">
        <img src="https://akhilpandey95.github.io/img/dr_model_list.png" />
    </a>
    <figcaption>
        <p>Fig 1. Open deep research model list observed from DR Tulu web annoucement page. <a href="https://allenai.org/blog/dr-tulu" target="_blank">Learn more</a></p>
    </figcaption>
</figure>
<p>For those interested in quickly training a deep-research model that is similar to <em>DR Tulu</em>, the instructions <a href="https://github.com/rlresearch/dr-tulu/tree/main/rl/open-instruct">here</a> highlight that testing the setup using <strong>Qwen-3 0.6B</strong> on a single (assuming H100) gpu is possible.</p>
<p><em>Tongyi DeepResearch</em> takes a different approach to the deep research problem. It&rsquo;s built on a Mixture-of-Experts (MoE) architecture with 30.5B total parameters but only 3.3B activated per token <a href="https://arxiv.org/abs/2510.24701">[5]</a>. The training pipeline is elaborate: three distinct stages (Agentic CPT, SFT, RL) with a data synthesis framework called AgentFounder that generates training data from knowledge graphs. What stands out in their benchmarks is the performance on GAIA and AssistantBench&ndash;tasks that require multi-step reasoning across multiple sources. At inference time, you can switch between a lightweight ReAct mode for quick queries and IterResearch mode for thorough investigation, which feels like a practical acknowledgment that not every question needs the same depth.</p>
<figure>
    <a href="https://akhilpandey95.github.io/img/dr_tongyi_results.png" target="_blank">
        <img src="https://akhilpandey95.github.io/img/dr_tongyi_results.png" />
    </a>
    <figcaption>
        <p>Fig 2. Tongyi DeepResearch agentic model benchmark results on several search benchmarks. <a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/" target="_blank">More available</a></p>
    </figcaption>
</figure>
<p><em>Kosmos</em> from Edison Scientific is architecturally the most distinctive of the three. Rather than betting on larger context windows or more sophisticated prompting, they built what they call a &ldquo;structured world model&rdquo; a persistent, queryable database of entities, relationships, and open questions that survives across runs<a href="https://arxiv.org/abs/2511.02824">[6]</a>. The system coordinates a data analysis agent and a literature search agent that share information through this store. The scale of operation is striking: a single research run can execute 42,000 lines of code and read 1,500 full papers. Their eval claims that one 20-cycle run was equivalent to 6 months of a human collaborator&rsquo;s research time<a href="https://edisonscientific.com/articles/announcing-kosmos">[3]</a>. Perhaps more provocatively, they claim four novel contributions to scientific literature emerged from Kosmos runs if that holds up under scrutiny, it&rsquo;s a meaningful shift from aggregation to generation.</p>
<figure>
    <a href="https://akhilpandey95.github.io/img/kosmos.png" target="_blank">
        <img src="https://akhilpandey95.github.io/img/kosmos.png" />
    </a>
    <figcaption>
        <p>Fig 3. Input to output pipeline for a scientific world model involved in sophisticated autonomous discovery. More about Kosmos (https://edisonscientific.com/articles/announcing-kosmos)
    </figcaption>
</figure>
<h3 id="structurally-are-they-different-tho-">Structurally, are they different tho ?<a hidden class="anchor" aria-hidden="true" href="#structurally-are-they-different-tho-">#</a></h3>
<p>The appeal of a <em>deep-research</em> system comes down to how well it juggles searching, reading, and synthesizing. Looking at these three systems side by side, the philosophical divergences become clear.</p>
<p>While it can seem as the core tradeoff is between <em>implicit knowledge</em> vs <em>explicit knowledge</em> the RLER approach keeps evolving rubrics during training, dropping ones that hit near-zero reward variance because you don&rsquo;t need a large complex model here if your training signal (for this specific task obviously) is good enough. I think i was very firm on this notion that if you treat this abstraction of <code>think</code>, <code>call_tool</code>, <code>cite</code>, <code>answer</code> <a href="https://arxiv.org/abs/2511.19399">[4]</a> as a protocol, then training a model (like DR Tulu) for this <em>task</em> seems to be pretty straight forward to achieve good results.</p>
<p>I guess operating with the above viewpoint made me question my core intuition when i noticed Tongyi&rsquo;s model since it takes the opposite intuition because the report suggests scale and efficiency are the right choices for a performant model. The three stage training pipeline (Agentic CPT, SFT, RL) suggests they view deep research capability as something that needs to be layered in progressively rather than emerging from a single training objective.</p>
<p>Now it might feel like I&rsquo;m forcing in Kosmos into this discussion but if you give me a minute, I&rsquo;d like to say that Kosmos sidesteps the above intuition entirely. Instead of asking &ldquo;how do we fit more knowledge into the model?&rdquo;, they ask &ldquo;why does knowledge need to live in the model at all?&rdquo; The structured world model is essentially an admission that context windows are a fundamentally limited abstraction for research tasks. When you&rsquo;re synthesizing across hundreds of papers, you need something that persists and can be queried structurally. While Kosmos might be a commercial example of scientific product with structural difference on the appeal of a <em>deep-research</em> system, it&rsquo;s interesting is how these three examples align with different use cases.</p>
<p>DR Tulu&rsquo;s lightweight approach makes it accessible (1 gpu) and focussed on a straight forward task. Tongyi&rsquo;s model is more agentic and its work horse nature lets you match depth to task complexity while using the deep-research mode. Kosmos is overkill for simple queries but potentially necessary for scientific discovery or rather this promise of &ldquo;scientific acceleration*. While this is not an classification encompassing all of the models/systems, these above three are definitely three interesting intuitions of a <em>deep-research</em> system that have an underlying abstraction thats possible to evolve.</p>
<h3 id="personally-i-feel">Personally, I feel<a hidden class="anchor" aria-hidden="true" href="#personally-i-feel">#</a></h3>
<p>As a researcher, the biggest cognitive boost I can receive is by having a reliable co-scientist capable of understanding my workflows for consumption (web), knowledge updates (memory), and selective recall of consumed information (skills) at frequent/infrequent intervals such that I can play a productive role in <em>directing</em> research rather than drowning in its logistics.</p>
<p>What excites me about the current landscape isn&rsquo;t any single model&ndash;its the patterns emerging across all of them. The Kosmos eval where a single 20-cycle run was equivalent to 6 months of a collaborator&rsquo;s research time<a href="https://arxiv.org/abs/2511.02824">[6]</a> is cool, but <em>how</em> it gets there is cooler: a structured world model that just doesn&rsquo;t forget. This hits at what I think is my biggest bottleneck particularly, the cognitive overhead of re-establishing context every time I pick up a thread. To be honest, I spend a non-trivial amount of time re-reading papers I&rsquo;ve already read, re-deriving conclusions I&rsquo;ve already drawn, rediscovering connections I already made. A system with persistent, queryable memory feels less like a tool and more like an extension of my episodic memory that doesn&rsquo;t decay.</p>
<p>A convincingly written but wrong paragraph is arguably worse than no output, so I kinda like systems where I can trace where claims come from, so DR Tulu&rsquo;s explicit <code>cite</code> token and Tongyi&rsquo;s emphasis on faithful citations matter more to me than benchmark numbers. The ideal co-scientist for me would probably a frankenstein blend of Kosmos&rsquo;s structured memory with DR Tulu&rsquo;s citation first approach and Tongyi&rsquo;s inference flexibility. We&rsquo;re not there yet (or we are and I&rsquo;m unaware), but all three being open-weight means the community can iterate toward that.</p>
<p>It would be fascinating and super scary to see these systems move from <em>aggregation</em> to <em>hypothesis generation</em> that actually surprises domain experts. Kosmos claims four novel contributions to scientific literature<a href="https://edisonscientific.com/articles/announcing-kosmos">[3]</a> and if these contributions really hold up, we&rsquo;re seeing the early stages of 2026 vibe-researcher / scientific acceleration phenomenon.</p>
<h3 id="the-idea-of-deep-is-evolving">The idea of &ldquo;deep&rdquo; is evolving<a hidden class="anchor" aria-hidden="true" href="#the-idea-of-deep-is-evolving">#</a></h3>
<p><em>Deep-research</em> as a concept is evolving and abstracted into many interesting ways. Google&rsquo;s <a href="https://ai.google.dev/gemini-api/docs/deep-research">Gemini Deep Research</a><a href="https://ai.google.dev/gemini-api/docs/deep-research">[7]</a> are some of the early examples of this evolving trend of deep agents powered by a strong foundational model capable on running long horizon tasks asynchronously through cycles of <code>Plan -&gt; Search -&gt; Read -&gt; Iterate -&gt; Output</code>. There are many variations of low-moderate level cognitive task that simply require longer horizons and this can be market analysis, scientific due diligence for drug discovery, carefull literature reviews for researchers to write a survey report/related work section in a paper, effectively any place where depth matters more than latency.</p>
<p>Also, its worth noting that open software SDKs/platforms are quickly converging on this notion because back in July'25 <a href="https://blog.langchain.com/deep-agents/">Deep Agents</a><a href="https://blog.langchain.com/deep-agents/">[8]</a> was launched effectively giving proper software scaffolding for abstracting the idea of <em>deep-agents</em>. Ideally, its easy to have seen this coming given major strides (i mean culturally) in 2025 have come from gemini deep research, TUIs (claude code, codex), manus computer use agents and several other agentic products. <code>deepagents</code> package simply packages these into a reusable software scaffolding/harness with <code>write_todos</code> for task decomposition, file system tools (<code>ls</code>, <code>read_file</code>, <code>write_file</code>, <code>edit_file</code>) for offloading context to disk, and a <code>task</code> tool for spawning specialized subagents with context isolation.</p>
<p>Most crucial emerging pattern i observe here is, <em>deep-research</em> being less about a single model (or) chat platform offering a button with &ldquo;deep-research&rdquo; but a gateway into a more sophisticated <em>interaction pattern</em> (or paradigm maybe). This slowly gives us a speak into 2026 on how enterprise software is going to reign in on long runnign agentic tasks; Long-running async execution, explicit planning phases, external memory stores, spawnable sub-agents. While its hard to see if open-source/open-weight efforts like DR Tulu, Tongyi will catch up to this evolving abstraction, a relevant point here is the seamless interchangable nature of &ldquo;deep research&rdquo; will practically define &ldquo;what is a useful LLM?&rdquo; into something that can actually do research rather than just answer questions about research.</p>
<h3 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h3>
<ol>
<li><a href="https://allenai.org/blog/dr-tulu">https://allenai.org/blog/dr-tulu</a></li>
<li><a href="https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/">https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/</a></li>
<li><a href="https://edisonscientific.com/articles/announcing-kosmos">https://edisonscientific.com/articles/announcing-kosmos</a></li>
<li><a href="https://arxiv.org/abs/2511.19399">https://arxiv.org/abs/2511.19399</a></li>
<li><a href="https://arxiv.org/abs/2510.24701">https://arxiv.org/abs/2510.24701</a></li>
<li><a href="https://arxiv.org/abs/2511.02824">https://arxiv.org/abs/2511.02824</a></li>
<li><a href="https://ai.google.dev/gemini-api/docs/deep-research">https://ai.google.dev/gemini-api/docs/deep-research</a></li>
<li><a href="https://blog.langchain.com/deep-agents/">https://blog.langchain.com/deep-agents/</a></li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://akhilpandey95.github.io/">Akhil Pandey Akella</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>

